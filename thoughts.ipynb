{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out how to import or what imports to use was extremely difficult. Mainly, because we needed a Data Collator that does a very niche thing that is not built in. So we needed to use TRL's DataCollatorForCompletionOnlyLM. I will try to explain this in more detail later. But for now I can share the import code that worked well. \n",
    "\n",
    "```\n",
    "%%capture\n",
    "import os\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
    "!pip install --upgrade -qqq uv\n",
    "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "except: get_numpy = \"numpy\"\n",
    "try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "except: is_t4 = False\n",
    "get_vllm, get_triton = (\"vllm==0.10.1\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
    "!uv pip install -qqq --upgrade     unsloth {get_vllm} {get_numpy} torchvision bitsandbytes xformers\n",
    "!uv pip install -qqq {get_triton}\n",
    "!uv pip install \"huggingface_hub>=0.34.0\" \"datasets>=3.4.1,<4.0.\n",
    "!uv pip install transformers==4.55.4\n",
    "!uv pip install \"trl==0.9.6\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might be wondering what all of this does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data creation is an interesting case. Because we need to do several things. We first need to process each task into a form that we intuitively think LLMs will have an easier time understanding. \n",
    "\n",
    "The original grind world problems are represented by JSON. So we have a bunch of unique task keys.\n",
    "\n",
    "So for each task the task data looks like this:\n",
    "```\n",
    "{test: {input: gridArray} ,\n",
    " train: [\n",
    "    {input: gridArray,\n",
    "     output: gridArray},\n",
    "    {input: gridArray,\n",
    "     output: gridArray}, ...\n",
    " ]}\n",
    "```\n",
    "and then the solution data looks like this:\n",
    "```\n",
    "[gridArray]\n",
    "```\n",
    "Note: sometimes there are multiple test inputs and solution outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to transform this into something that the ML model will have a less messy, easier time reading and have the proper format for our data loaders in training to be able to us. \n",
    "\n",
    "Note: we know what we want it to look like so now we just have to figure out what our data loader would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a very difficult part of the pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay what am I trying to do.\n",
    "I want to get the fine-tuning pipeline build using multiple gpus.\n",
    "I want to use somewhat basic configs so they are simple for me to understand.\n",
    "I want to have a clear file system that is easy to navigate\n",
    "The end goal is to have a fine-tuned model sent to my huggingface folder\n",
    "I think I will have to merge the weights together before sending them to huggingface\n",
    "I need the data to be created and then sent through the pipeline. I want to do DDP.\n",
    "I should figure out if they create the data and then send put it in the data collator or what\n",
    "So I need to set up data infra training infra and merging and sending infra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/shared/arc/\n",
    "├── data/                       # read-only datasets (everyone can read)\n",
    "│   └── ARC-Data/\n",
    "│       └── input/\n",
    "│           ├── arc-prize-2024/\n",
    "│           │   ├── arc-agi_evaluation_challenges.json\n",
    "│           │   └── arc-agi_evaluation_solutions.json\n",
    "│           ├── re_arc/\n",
    "│           │   ├── metadata.json\n",
    "│           │   └── tasks/\n",
    "│           │       ├── <task-id-1>.json\n",
    "│           │       └── ...\n",
    "│           └── arc-dataset-collection/\n",
    "│               └── dataset/\n",
    "│                   └── ConceptARC/\n",
    "│                       └── data/\n",
    "│                           ├── group-1/*.json\n",
    "│                           └── ...\n",
    "├── outputs/                    # shared, writable (checkpoints, logs, merged models)\n",
    "│   ├── runs/                   # per-run folders\n",
    "│   ├── checkpoints/\n",
    "│   ├── logs/\n",
    "│   └── models/\n",
    "│       └── Llama-3/            # where your script saves LoRA + merged\n",
    "└── cache/\n",
    "    ├── hf/                     # shared HF cache (optional)\n",
    "    └── ds/                     # optional DeepSpeed NVMe/AIO cache (if used)\n",
    "\n",
    "main/\n",
    "├── code/\n",
    "│   └── arc-trainer/            # your git repo (your Python package + scripts)\n",
    "│       ├── train_v1.py\n",
    "│       ├── ds_configs/\n",
    "│       │   ├── ds_ze[email protected]          # deepspeed jsons\n",
    "│       │   └── ds_zero2.json\n",
    "│       ├── configs/\n",
    "│       │   └── training.yaml   # hyperparams you may want in YAML\n",
    "│       ├── scripts/\n",
    "│       │   ├── run_slurm.sh    # SLURM launcher\n",
    "│       │   └── run_local.sh    # single-node launcher\n",
    "│       └── README.md\n",
    "└── .cache/\n",
    "    └── huggingface/            # per-user HF cache (if not using shared)\n",
    "\n",
    "/scratch/$USER/                 # fast temp (per-job)\n",
    "└── arc-run-<jobid>/            # created by run script at runtime\n",
    "    ├── offload/                # DS CPU/NVMe offload (if enabled)\n",
    "    └── tmp/                    # temp files\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
